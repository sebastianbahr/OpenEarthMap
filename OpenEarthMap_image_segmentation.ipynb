{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86b5add1-120a-4365-bc1e-7ce2fa3d09bf",
   "metadata": {},
   "source": [
    "# Semantic image segmentation using OpenEarthMap imagery and U-Net with Efficient Net B4 backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d33eb160-061b-42e7-8fee-8fac55ecaf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr as pyr\n",
    "from sys import getsizeof\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "from skimage import io\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import keras\n",
    "import segmentation_models as sm # \n",
    "from keras.metrics import MeanIoU\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from patchify import patchify\n",
    "\n",
    "sm.set_framework('tf.keras')\n",
    "sm.framework()\n",
    "random_seed = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ead955-44b7-4dec-a9db-35683f6fe772",
   "metadata": {},
   "source": [
    "## Import OpenEarthMap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04356a85-2a18-4c1c-ba40-ffb6c63e410b",
   "metadata": {},
   "source": [
    "The data can be downloaded from the following [website](https://open-earth-map.org/). The image segmentation masks contain 9 categories. Most of the images have a size of 1024x1024. However, some have the size 1000x1000 or 650x650. After loading the data a preprocessing pipeline is used to transform all images to a size of 512x512. Images of size 1024x1024 are patchified into four imags of equal size. To images of size 1000x1000 zero padding is added and they are patchified into four imags of equal size. If the image size is 650x650 a central crop is applied. This results in 5629 images and masks from developing countries and 3649 from developed countries and a total of 9278 examples. Due to the small size of the data set 5-fold cross validation is used later on to evaluate the models performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5011565-04bc-4042-8968-7c0d60873cd3",
   "metadata": {},
   "source": [
    "* 0 Unknown\n",
    "* 1 Bareland\n",
    "* 2 Rangeland\n",
    "* 3 Developed space\n",
    "* 4 Road\n",
    "* 5 Trees\n",
    "* 6 Water\n",
    "* 7 Agriculture land\n",
    "* 8 Buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cd671b0-b6c7-4f7c-af6e-10c498110725",
   "metadata": {},
   "outputs": [],
   "source": [
    "developing_countries = [\"abancay\",\"accra\",\"al_qurnah\",\"baybay\",\"bogota\",\"buenos_aires\",\"chiangmai\",\"chiclayo\",\"chincha\",\"coxsbazar\",\n",
    "                        \"daressalaam\",\"dhaka\",\"dowa\",\"ica\",\"kagera\",\"kampala\",\"khartoum\",\"kyoto\",\"lambayeque\",\"lima\",\"lohur\",\"mahe\",\n",
    "                        \"maputo\",\"monrovia\",\"ngaoundere\",\"niamey\",\"pisco\",\"piura\",\"pointenoire\",\"rosario\",\"san_tome\",\"santiago\",\n",
    "                        \"sechura\",\"soriano\",\"svaneti\",\"tokyo\",\"tonga\",\"ulaanbaatar\",\"viru\",\"western\",\"zanzibar\"]\n",
    "\n",
    "developed_countries = [\"aachen\",\"austin\",\"bielefeld\",\"chicago\",\"chisinau\",\"christchurch\",\"dolnoslaskie\",\"dortmund\",\"duesseldorf\",\"koeln\",\n",
    "                       \"tyrolw\",\"kujawsko-pomorskie\",\"mazowieckie\",\"melbourne\",\"podkarpackie\",\"podlaskie\",\"pomorskie\",\"rotterdam\",\n",
    "                       \"slaskie\",\"swietokrzyskie\",\"vienna\",\"warminsko-mazurskie\",\"wielkopolskie\",\"zachodniopomorskie\",\"paris\",\"shanghai\",\"vegas\"]\n",
    "\n",
    "all_countries = developing_countries+developed_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea42969c-e899-474d-a843-4202d8efa786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_and_image(path, valid_images, city):\n",
    "    imgs = []\n",
    "    img_names = []\n",
    "    img_masks = []\n",
    "\n",
    "    for f in os.listdir(path+city+\"/labels/\"):\n",
    "        ext = os.path.splitext(f)[1]\n",
    "        if ext.lower() not in valid_images:\n",
    "            continue\n",
    "        img_names.append(f)\n",
    "        img_masks.append(io.imread(os.path.join(path+city+\"/labels/\",f)))\n",
    "\n",
    "    for i in img_names:\n",
    "        imgs.append(io.imread(path+city+\"/images/\"+i))\n",
    "    \n",
    "    return img_names, img_masks, imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd82570e-518b-4782-8d30-8020bf869144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abancay\n",
      "accra\n",
      "al_qurnah\n",
      "baybay\n",
      "bogota\n",
      "buenos_aires\n",
      "chiangmai\n",
      "chiclayo\n",
      "chincha\n",
      "coxsbazar\n",
      "daressalaam\n",
      "dhaka\n",
      "dowa\n",
      "ica\n",
      "kagera\n",
      "kampala\n",
      "khartoum\n",
      "kyoto\n",
      "lambayeque\n",
      "lima\n",
      "lohur\n",
      "mahe\n",
      "maputo\n",
      "monrovia\n",
      "ngaoundere\n",
      "niamey\n",
      "pisco\n",
      "piura\n",
      "pointenoire\n",
      "rosario\n",
      "san_tome\n",
      "santiago\n",
      "sechura\n",
      "soriano\n",
      "svaneti\n",
      "tokyo\n",
      "tonga\n",
      "ulaanbaatar\n",
      "viru\n",
      "western\n",
      "zanzibar\n",
      "aachen\n",
      "austin\n",
      "bielefeld\n",
      "chicago\n",
      "chisinau\n",
      "christchurch\n",
      "dolnoslaskie\n",
      "dortmund\n",
      "duesseldorf\n",
      "koeln\n",
      "tyrolw\n",
      "kujawsko-pomorskie\n",
      "mazowieckie\n",
      "melbourne\n",
      "podkarpackie\n",
      "podlaskie\n",
      "pomorskie\n",
      "slaskie\n",
      "swietokrzyskie\n",
      "vienna\n",
      "warminsko-mazurskie\n",
      "wielkopolskie\n",
      "zachodniopomorskie\n",
      "paris\n",
      "shanghai\n",
      "vegas\n"
     ]
    }
   ],
   "source": [
    "city_names=[]\n",
    "city_masks=[]\n",
    "city_imgs=[]\n",
    "\n",
    "for i in all_countries:\n",
    "    c_names, c_masks, c_imgs = get_mask_and_image(\"//resstore.unibe.ch/sowi_research_bahr/openearthmap/\", [\".tif\"], i)\n",
    "    \n",
    "    if c_imgs[0].shape == (1024,1024,3):\n",
    "        for img in c_imgs:\n",
    "            patchified_img = patchify(img, (512, 512, 3), step=512)\n",
    "            for k in range(patchified_img.shape[0]):\n",
    "                for j in range(patchified_img.shape[1]):\n",
    "                    city_imgs.append(patchified_img[k, j,0,:, :, :]) \n",
    "            \n",
    "        for mask in c_masks:\n",
    "            patchified_mask = patchify(np.expand_dims(mask, axis=2), (512, 512, 1), step=512)           \n",
    "            for k in range(patchified_mask.shape[0]):\n",
    "                for j in range(patchified_mask.shape[1]):\n",
    "                    city_masks.append(patchified_mask[k, j,0,:, :, :])\n",
    "            \n",
    "        for n in c_names:\n",
    "            for n_i in range(0, 4):\n",
    "                city_names.append(n.split(\".\")[0])\n",
    "        print(i)\n",
    "        \n",
    "    elif c_imgs[0].shape == (1000,1000,3):\n",
    "        for n in c_names:\n",
    "            for n_i in range(0, 4):\n",
    "                city_names.append(n.split(\".\")[0])             \n",
    "          \n",
    "        for img in c_imgs:\n",
    "            c_img = np.pad(img, pad_width=[(12, 12),(12, 12),(0, 0)], mode='constant', constant_values=0)\n",
    "            patchified_img = patchify(c_img, (512, 512, 3), step=512) \n",
    "            for k in range(patchified_img.shape[0]):\n",
    "                for j in range(patchified_img.shape[1]):\n",
    "                    city_imgs.append(patchified_img[k, j,0,:, :, :])\n",
    "            \n",
    "        for mask in c_masks:                \n",
    "            c_mask = np.pad(np.expand_dims(mask, axis=2), pad_width=[(12, 12),(12, 12),(0, 0)], mode='constant', constant_values=0)\n",
    "            patchified_mask = patchify(c_mask, (512, 512, 1), step=512)\n",
    "            for k in range(patchified_mask.shape[0]):\n",
    "                for j in range(patchified_mask.shape[1]):\n",
    "                    city_masks.append(patchified_mask[k, j,0,:, :, :])\n",
    "        print(i)\n",
    "    \n",
    "    elif c_imgs[0].shape == (650,650,3):\n",
    "        for img in c_imgs:        \n",
    "            city_imgs.append(np.array(tf.image.central_crop(img, 0.787)))\n",
    "            \n",
    "        for mask in c_masks:\n",
    "            city_masks.append(np.array(tf.image.central_crop(np.expand_dims(mask, axis=2), 0.787)))\n",
    "            \n",
    "        for name in c_names:\n",
    "            city_names.append(name.split(\".\")[0])\n",
    "        print(i)\n",
    "            \n",
    "        \n",
    "\n",
    "city_masks = np.array(city_masks)\n",
    "city_imgs = np.array(city_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a361b-ece9-4de3-af9a-e2d0e50cee72",
   "metadata": {},
   "source": [
    "## Distribution of mask segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e144907-1152-44fd-ad4a-a665228b30ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "value, count = np.unique(np.array(city_masks).flatten(), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5fab09b-26ee-4fac-95bf-5d7c03d398b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar([str(x) for x  in value], (count/np.sum(count))*100)\n",
    "plt.xlabel(\"Mask segments\")\n",
    "plt.ylabel(\"Proportion in %\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6e8a58-c8df-4899-af3b-4837ecb77802",
   "metadata": {},
   "source": [
    "## Distribution of mask segments in developed countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c314cab4-ae29-48fa-a8f7-1385cd26b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "isov = pd.read_excel(\"H:/Sebastian_Bahr/PhD/2_Publication/Crime_and_ML/Data/image_segmentation_city_overview.xlsx\")\n",
    "\n",
    "names = pd.DataFrame([x.rsplit(\"_\")[0] for x in list(city_names)], columns=[\"city\"])\n",
    "names = names.merge(isov[[\"city\", \"developed\"]], on=\"city\", how=\"left\")\n",
    "idx_developed = names[names.developed==1].index  \n",
    "city_masks_developed = city_masks[idx_developed]\n",
    "\n",
    "value, count = np.unique(np.array(city_masks_developed).flatten(), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dc256bf-2365-41c2-97c1-2cd129fdfe73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar([str(x) for x  in value], (count/np.sum(count))*100)\n",
    "plt.xlabel(\"Mask segments\")\n",
    "plt.ylabel(\"Proportion in %\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f497ec-fbc1-4e67-bf0f-802d9b78cf8e",
   "metadata": {},
   "source": [
    "## Encode mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d53c3f3-2cac-40a5-917c-256a0191c076",
   "metadata": {},
   "source": [
    "Encode the 1 dimensional to a 9 dimensional mask using one-hot-encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c892aef9-5718-4d06-8a8c-593ffe3f0b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bahr\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "n, h, w = city_masks[:,:,:,0].shape\n",
    "city_masks_reshaped = np.array(city_masks).reshape(-1,1)\n",
    "city_masks_reshaped_encoded = labelencoder.fit_transform(city_masks_reshaped)\n",
    "city_masks_encoded = city_masks_reshaped_encoded.reshape(n, h, w)\n",
    "city_masks_input = np.expand_dims(city_masks_encoded, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "392d2a5c-9350-47f4-b3fd-f526134c601c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9278, 512, 512, 9)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = 9\n",
    "city_masks_cat = tf.keras.utils.to_categorical(city_masks_input, num_classes=n_classes)\n",
    "city_masks_cat = city_masks_cat.astype(\"uint8\")\n",
    "\n",
    "# check shape\n",
    "city_masks_cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae05b0ae-ea16-4a62-bf87-b8ec9e8988dd",
   "metadata": {},
   "source": [
    "## Randomly shuffle the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09d459c1-999e-448f-a02b-d44b0e8df569",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_seed)\n",
    "idx = np.random.permutation(len(city_imgs))\n",
    "city_names_shuffled = np.array(city_names)[idx]\n",
    "city_imgs_shuffled = city_imgs[idx]\n",
    "city_masks_cat_shuffled = city_masks_cat[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f178e0-5aca-49a0-9d79-f26bc26752f2",
   "metadata": {},
   "source": [
    "## 5-fold CV data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81419455-6fc6-42b3-b45c-66c2d87cd139",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "i=1\n",
    "kfold = KFold(n_splits=num_folds, shuffle=False)\n",
    "for idx_train, idx_test in kfold.split(city_imgs_shuffled, city_masks_cat_shuffled, city_names_shuffled):   \n",
    "    train_imgs = city_imgs_shuffled[idx_train]\n",
    "    train_mask = city_masks_cat_shuffled[idx_train]\n",
    "    test_imgs = city_imgs_shuffled[idx_test]\n",
    "    test_mask = city_masks_cat_shuffled[idx_test]\n",
    "    test_names = city_names_shuffled[idx_test]\n",
    "    np.save(\"/path/CV%s_train_imgs_all_patch\" %i, train_imgs)\n",
    "    np.save(\"/path/CV%s_train_mask_all_patch\" %i, train_mask)\n",
    "    np.save(\"/path/CV%s_test_names_all_patch\" %i, test_names)\n",
    "    np.save(\"/path/CV%s_test_imgs_all_patch\" %i, test_imgs)\n",
    "    np.save(\"/path/CV%s_test_mask_all_patch\" %i, test_mask)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9b6fed-5326-4c09-8759-6b258e252d46",
   "metadata": {},
   "source": [
    "## Preprocess the data and compile the models in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d2afa-4724-483b-939a-f2675d98b756",
   "metadata": {},
   "source": [
    "Due to the high requirement of VRAM the models were trained on the on [UBELIX](http://www.id.unibe.ch/hpc), the HPC cluster at the University of Bern. The same process is repeated for folds 2-5. Most of the hyperparameters are choosen following the suggestion of the original paper of [Xia et al. 2022](https://arxiv.org/abs/2210.10732). However, multiple experiments were run using different backbones and losses. In particulare, the backbone based on an EfficientNet B4 outperformed backbones based on Resnet 34, Resnext 101 and EfficientNet B0. Further, summing focal and jaccard loss outperformed categorical cross entropy, dice loss and combinations of them with focal loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bef8ac-1e8b-467a-b83a-9af709174354",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set hyperparameter and variables\n",
    "i = \"effnetb4_dice_1\" # MC name\n",
    "m = \"effnetb4\" # name backbone\n",
    "batchsize = 8\n",
    "prefetch = 1\n",
    "n_classes = 9\n",
    "n_epoch = 120\n",
    "lr = 0.0001\n",
    "decay = 0.000001\n",
    "metrics = [tf.keras.metrics.CategoricalAccuracy(name=\"cat_accuracy\"),\n",
    "           sm.metrics.IOUScore(threshold=0.5, class_indexes=[1,2,3,4,5,6,7,8]),\n",
    "           sm.metrics.FScore(threshold=0.5, class_indexes=[1,2,3,4,5,6,7,8]),\n",
    "           tf.keras.metrics.MeanIoU(num_classes=n_classes)]\n",
    "\n",
    "\n",
    "## Load data CV1\n",
    "CV1_train_imgs = np.load(\"/path/CV1_train_imgs_all_patch.npy\")\n",
    "CV1_train_mask = np.load(\"/path/CV1_train_mask_all_patch.npy\")\n",
    "CV1_test_imgs = np.load(\"/path/CV1_test_imgs_all_patch.npy\")\n",
    "CV1_test_mask = np.load(\"/path/CV1_test_mask_all_patch.npy\")\n",
    "\n",
    "\n",
    "## Data preprocessing\n",
    "def preprocess_images(image, label):\n",
    "    label = tf.cast(label, tf.float16)\n",
    "    image = tf.cast(image, tf.uint8)  \n",
    "    image = tf.keras.applications.efficientnet.preprocess_input(image)\n",
    "    image = tf.clip_by_value(image, 0, 255)  \n",
    "    return(image, label)\n",
    "\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((CV1_train_imgs, CV1_train_mask))\n",
    "    train_data = train_data.batch(batchsize, drop_remainder=False)\n",
    "    train_data = train_data.map(preprocess_images, num_parallel_calls=8)  \n",
    "    \n",
    "    test_data = tf.data.Dataset.from_tensor_slices((CV1_test_imgs, CV1_test_mask))\n",
    "    test_data = test_data.batch(batchsize, drop_remainder=False)\n",
    "    test_data = test_data.map(preprocess_images, num_parallel_calls=8)\n",
    "    \n",
    "\n",
    "with tf.device('/cpu:0'):   \n",
    "    ## Define loss    \n",
    "    dice_loss = sm.losses.DiceLoss(class_indexes=[1,2,3,4,5,6,7,8]) \n",
    "    \n",
    "    \n",
    "with tf.device('/cpu:0'):  \n",
    "    ## Load U-net backbone\n",
    "    backbone_model = keras.models.load_model(\"/path/u_net_models/model_u_%s_image_net_weights.hdf5\" %m)\n",
    "    \n",
    "    \n",
    "    ## Define optimizer\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=decay)\n",
    "    \n",
    "    \n",
    "    ## Compile model\n",
    "    backbone_model.compile(optimizer=optimizer,\n",
    "                            loss=dice_loss, \n",
    "                            metrics=metrics)\n",
    "    \n",
    "    ## Callbacks\n",
    "    mc = [tf.keras.callbacks.ModelCheckpoint(filepath=\"/path/u_net_models_mc/u_net_%s\" %i,\n",
    "                                             save_best_only=True, monitor='val_iou_score', mode='max')] \n",
    "    \n",
    "    \n",
    "## Train model\n",
    "with tf.device('/gpu:0'):\n",
    "    hist = backbone_model.fit(train_data,\n",
    "                              validation_data=test_data,\n",
    "                              batch_size=batchsize,\n",
    "                              epochs=n_epoch,\n",
    "                              callbacks=[mc],\n",
    "                              verbose = 0)\n",
    "    \n",
    "    \n",
    "## Model evaluation\n",
    "with tf.device('/cpu:0'): \n",
    "    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    \n",
    "    def plot_metrics(history):\n",
    "        metrics = ['loss', 'cat_accuracy', 'iou_score', 'f1-score']\n",
    "        for n, metric in enumerate(metrics):\n",
    "            name = metric.replace(\"_\",\" \").capitalize()\n",
    "            plt.subplot(2,2,n+1)\n",
    "            plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
    "            plt.plot(history.epoch, history.history['val_'+metric],\n",
    "                 color=colors[0], linestyle=\"--\", label='Val')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel(name)\n",
    "            if metric == 'loss':\n",
    "                plt.ylim([0, plt.ylim()[1]])\n",
    "            elif metric == 'auc':\n",
    "                plt.ylim([0.5,1])\n",
    "            else:\n",
    "                plt.ylim([0,1])\n",
    "                \n",
    "            plt.legend();\n",
    "            \n",
    "            \n",
    "    plot_metrics(hist)\n",
    "    plt.savefig(\"/path/performance_plots/u_net_%s\" %i)       \n",
    "      \n",
    "    \n",
    "    backbone_model.compile(optimizer=optimizer,\n",
    "                           loss=dice_loss,\n",
    "                           metrics=metrics)\n",
    "    backbone_model.load_weights(\"/path/u_net_models_mc/u_net_%s\" %i)\n",
    "            \n",
    "       \n",
    "    print(backbone_model.evaluate(test_data, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d769f7df-5a46-4572-b10d-71ef984891ff",
   "metadata": {},
   "source": [
    "## Test the model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdcfbc1-7ed2-4c7e-82ae-0a72a004f362",
   "metadata": {},
   "source": [
    "First the model's performance by class and the average over all classes is evaluated. Second, Test Time Augmentaion is used to increase the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8759bffe-a14a-4e0a-8009-5b011b7cd250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IoU_by_class(true, pred, class_, threshold=0.5):\n",
    "    iou_scores=[]\n",
    "    classes=[]\n",
    "    for i in class_:\n",
    "        smooth = 1e-5\n",
    "        true_ = true[:,:,:,i]\n",
    "        pred_ = pred[:,:,:,i]\n",
    "        pred_ = tf.keras.backend.greater(pred_, threshold)\n",
    "        pred_ = tf.cast(pred_, tf.float32)\n",
    "        true_ = tf.cast(true_, tf.float32)\n",
    "        intersection = np.logical_and(true_, pred_)\n",
    "        union = np.logical_or(true_, pred_)\n",
    "        iou_scores.append((np.sum(intersection)+smooth)/(np.sum(union)+smooth))\n",
    "        classes.append(i)\n",
    "    avg_iou = sum(iou_scores)/len(iou_scores)\n",
    "    return iou_scores + [avg_iou]\n",
    "\n",
    "\n",
    "def get_F1score_by_class(true, pred, class_, threshold=0.5):\n",
    "    f1_scores=[]\n",
    "    classes=[]\n",
    "    for i in class_:\n",
    "        smooth = 1e-5\n",
    "        true_ = true[:,:,:,i]\n",
    "        pred_ = pred[:,:,:,i]\n",
    "        pred_ = tf.keras.backend.greater(pred_, threshold)\n",
    "        pred_ = tf.cast(pred_, tf.float32)\n",
    "        true_ = tf.cast(true_, tf.float32)\n",
    "        intersection = np.logical_and(true_, pred_)\n",
    "        union = np.logical_or(true_, pred_)\n",
    "        f1_scores.append(((2*np.sum(intersection))+smooth) / (np.sum(union)+np.sum(intersection)+smooth))\n",
    "        classes.append(i)\n",
    "    avg_iou = sum(f1_scores)/len(f1_scores)\n",
    "    return f1_scores + [avg_iou]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3f3179-56b1-4209-be8e-7fb033fd26ce",
   "metadata": {},
   "source": [
    "### Model's performance without TTA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de9e348-4567-4c08-8d8b-b532324a8df3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Model trained on CV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4c572e-c1fe-4249-a8d3-cbd847c1ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = \"effnetb4_focc_jacc_10\" # model name\n",
    "m = \"effnetb4\" # name backbone\n",
    "batchsize = 8\n",
    "n_classes = 9\n",
    "lr = 0.0001\n",
    "decay = 0.000001\n",
    "\n",
    "\n",
    "metrics = [tf.keras.metrics.CategoricalAccuracy(name=\"cat_accuracy\"),\n",
    "           sm.metrics.IOUScore(threshold=0.5, class_indexes=[1,2,3,4,5,6,7,8]),\n",
    "           sm.metrics.FScore(threshold=0.5, class_indexes=[1,2,3,4,5,6,7,8]),\n",
    "           tf.keras.metrics.MeanIoU(num_classes=n_classes)]\n",
    "\n",
    "\n",
    "# Load test data\n",
    "test_img = np.load(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/CV1_test_imgs_all_patch.npy\")\n",
    "test_mask = np.load(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/CV1_test_mask_all_patch.npy\")\n",
    "\n",
    "\n",
    "with tf.device('/cpu:0'):  \n",
    "    ## Load U-net backbone\n",
    "    backbone_model = keras.models.load_model(\"//resstore.unibe.ch/sowi_research_bahr/u_net_models/model_u_%s_image_net_weights.hdf5\" %m)\n",
    "    \n",
    "    \n",
    "    ## Define loss    \n",
    "    dice_loss = sm.losses.DiceLoss(class_indexes=[1,2,3,4,5,6,7,8]) \n",
    "    \n",
    "\n",
    "    ## Define optimizer\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=decay)\n",
    "    \n",
    "    \n",
    "    ## Compile model\n",
    "    backbone_model.compile(optimizer=optimizer,\n",
    "                            loss=dice_loss, \n",
    "                            metrics=metrics)\n",
    "    \n",
    "    \n",
    "    ## Load model weights\n",
    "    backbone_model.load_weights(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/u_net_models_mc/u_net_%s\" %i)\n",
    "    \n",
    "\n",
    "## Make predictions on test data\n",
    "pred_mask_CV1 = []\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    for i in range(0, len(test_img)):\n",
    "        pred_mask_CV1.append(backbone_model.predict(np.expand_dims(test_img[i], axis=0)))\n",
    "        \n",
    "        \n",
    "## get model's performance\n",
    "IoU_CV1 = get_IoU_by_class(test_mask, np.array(pred_mask_CV1)[:,0,:,:,:], [1,2,3,4,5,6,7,8])\n",
    "F1_CV1 = get_F1score_by_class(test_mask, np.array(pred_mask_CV1)[:,0,:,:,:], [1,2,3,4,5,6,7,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85295eae-04db-4fa0-862c-c6f10ca689e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Model trained on CV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59f7b35-d998-467d-a80d-07305aa105f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = \"effnetb4_focc_jacc_11\" # model name\n",
    "m = \"effnetb4\" # name backbone\n",
    "batchsize = 8\n",
    "n_classes = 9\n",
    "lr = 0.0001\n",
    "decay = 0.000001\n",
    "\n",
    "\n",
    "metrics = [tf.keras.metrics.CategoricalAccuracy(name=\"cat_accuracy\"),\n",
    "           sm.metrics.IOUScore(threshold=0.5, class_indexes=[1,2,3,4,5,6,7,8]),\n",
    "           sm.metrics.FScore(threshold=0.5, class_indexes=[1,2,3,4,5,6,7,8]),\n",
    "           tf.keras.metrics.MeanIoU(num_classes=n_classes)]\n",
    "\n",
    "\n",
    "# Load test data\n",
    "test_img = np.load(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/CV2_test_imgs_all_patch.npy\")\n",
    "test_mask = np.load(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/CV2_test_mask_all_patch.npy\")\n",
    "\n",
    "\n",
    "with tf.device('/cpu:0'):  \n",
    "    ## Load U-net backbone\n",
    "    backbone_model = keras.models.load_model(\"//resstore.unibe.ch/sowi_research_bahr/u_net_models/model_u_%s_image_net_weights.hdf5\" %m)\n",
    "    \n",
    "    \n",
    "    ## Define loss    \n",
    "    dice_loss = sm.losses.DiceLoss(class_indexes=[1,2,3,4,5,6,7,8])\n",
    "    \n",
    "\n",
    "    ## Define optimizer\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=decay)\n",
    "    \n",
    "    \n",
    "    ## Compile model\n",
    "    backbone_model.compile(optimizer=optimizer,\n",
    "                            loss=dice_loss, \n",
    "                            metrics=metrics)\n",
    "    \n",
    "    \n",
    "    ## Load model weights\n",
    "    backbone_model.load_weights(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/u_net_models_mc/u_net_%s\" %i)\n",
    "    \n",
    "\n",
    "## Make predictions on test data\n",
    "pred_mask_CV2 = []\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    for i in range(0, len(test_img)):\n",
    "        pred_mask_CV2.append(backbone_model.predict(np.expand_dims(test_img[i], axis=0)))\n",
    "        \n",
    "        \n",
    "## get model's performance\n",
    "IoU_CV2 = get_IoU_by_class(test_mask, np.array(pred_mask_CV2)[:,0,:,:,:], [1,2,3,4,5,6,7,8])\n",
    "F1_CV2 = get_F1score_by_class(test_mask, np.array(pred_mask_CV2)[:,0,:,:,:], [1,2,3,4,5,6,7,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa5cfb4-8df1-4991-a426-10a4eb0988ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Model trained on CV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab2a6a0-cabe-4750-b144-c6fc925d740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = \"effnetb4_focc_jacc_12\" # model name\n",
    "m = \"effnetb4\" # name backbone\n",
    "batchsize = 8\n",
    "n_classes = 9\n",
    "lr = 0.0001\n",
    "decay = 0.000001\n",
    "\n",
    "\n",
    "metrics = [tf.keras.metrics.CategoricalAccuracy(name=\"cat_accuracy\"),\n",
    "           sm.metrics.IOUScore(threshold=0.5, class_indexes=[1,2,3,4,5,6,7,8]),\n",
    "           sm.metrics.FScore(threshold=0.5, class_indexes=[1,2,3,4,5,6,7,8]),\n",
    "           tf.keras.metrics.MeanIoU(num_classes=n_classes)]\n",
    "\n",
    "\n",
    "# Load test data\n",
    "test_img = np.load(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/CV3_test_imgs_all_patch.npy\")\n",
    "test_mask = np.load(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/CV3_test_mask_all_patch.npy\")\n",
    "\n",
    "\n",
    "with tf.device('/cpu:0'):  \n",
    "    ## Load U-net backbone\n",
    "    backbone_model = keras.models.load_model(\"//resstore.unibe.ch/sowi_research_bahr/u_net_models/model_u_%s_image_net_weights.hdf5\" %m)\n",
    "    \n",
    "    \n",
    "    ## Define loss    \n",
    "    dice_loss = sm.losses.DiceLoss(class_indexes=[1,2,3,4,5,6,7,8]) \n",
    "    \n",
    "\n",
    "    ## Define optimizer\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=decay)\n",
    "    \n",
    "    \n",
    "    ## Compile model\n",
    "    backbone_model.compile(optimizer=optimizer,\n",
    "                            loss=dice_loss, \n",
    "                            metrics=metrics)\n",
    "    \n",
    "    \n",
    "    ## Load model weights\n",
    "    backbone_model.load_weights(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/u_net_models_mc/u_net_%s\" %i)\n",
    "    \n",
    "\n",
    "## Make predictions on test data\n",
    "pred_mask_CV3 = []\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    for i in range(0, len(test_img)):\n",
    "        pred_mask_CV3.append(backbone_model.predict(np.expand_dims(test_img[i], axis=0)))\n",
    "        \n",
    "        \n",
    "## get model's performance\n",
    "IoU_CV3 = get_IoU_by_class(test_mask, np.array(pred_mask_CV3)[:,0,:,:,:], [1,2,3,4,5,6,7,8])\n",
    "F1_CV3 = get_F1score_by_class(test_mask, np.array(pred_mask_CV3)[:,0,:,:,:], [1,2,3,4,5,6,7,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134275d3-7cad-4450-b33b-227d514b670d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Model trained on CV4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bb3866-b8f3-42b6-b4c1-50e2f7049e43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = \"effnetb4_focc_jacc_13\" # model name\n",
    "m = \"effnetb4\" # name backbone\n",
    "batchsize = 8\n",
    "n_classes = 9\n",
    "lr = 0.0001\n",
    "decay = 0.000001\n",
    "\n",
    "\n",
    "metrics = [tf.keras.metrics.CategoricalAccuracy(name=\"cat_accuracy\"),\n",
    "           sm.metrics.IOUScore(threshold=0.5, class_indexes=[1,2,3,4,5,6,7,8]),\n",
    "           sm.metrics.FScore(threshold=0.5, class_indexes=[1,2,3,4,5,6,7,8]),\n",
    "           tf.keras.metrics.MeanIoU(num_classes=n_classes)]\n",
    "\n",
    "\n",
    "# Load test data\n",
    "test_img = np.load(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/CV4_test_imgs_all_patch.npy\")\n",
    "test_mask = np.load(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/CV4_test_mask_all_patch.npy\")\n",
    "\n",
    "\n",
    "with tf.device('/cpu:0'):  \n",
    "    ## Load U-net backbone\n",
    "    backbone_model = keras.models.load_model(\"//resstore.unibe.ch/sowi_research_bahr/u_net_models/model_u_%s_image_net_weights.hdf5\" %m)\n",
    "    \n",
    "    \n",
    "    ## Define loss    \n",
    "    dice_loss = sm.losses.DiceLoss(class_indexes=[1,2,3,4,5,6,7,8]) \n",
    "    \n",
    "\n",
    "    ## Define optimizer\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=decay)\n",
    "    \n",
    "    \n",
    "    ## Compile model\n",
    "    backbone_model.compile(optimizer=optimizer,\n",
    "                            loss=dice_loss, \n",
    "                            metrics=metrics)\n",
    "    \n",
    "    \n",
    "    ## Load model weights\n",
    "    backbone_model.load_weights(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/u_net_models_mc/u_net_%s\" %i)\n",
    "    \n",
    "\n",
    "## Make predictions on test data\n",
    "pred_mask_CV4 = []\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    for i in range(0, len(test_img)):\n",
    "        pred_mask_CV4.append(backbone_model.predict(np.expand_dims(test_img[i], axis=0)))\n",
    "        \n",
    "        \n",
    "## get model's performance\n",
    "IoU_CV4 = get_IoU_by_class(test_mask, np.array(pred_mask_CV4)[:,0,:,:,:], [1,2,3,4,5,6,7,8])\n",
    "F1_CV4 = get_F1score_by_class(test_mask, np.array(pred_mask_CV4)[:,0,:,:,:], [1,2,3,4,5,6,7,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f87702-4f4d-434e-aa1e-e92f70e09ee9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Model trained on CV5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea140c68-0abb-45a7-b772-8f04679cb11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = \"effnetb4_focc_jacc_14\" # model name\n",
    "m = \"effnetb4\" # name backbone\n",
    "batchsize = 8\n",
    "n_classes = 9\n",
    "lr = 0.0001\n",
    "decay = 0.000001\n",
    "\n",
    "\n",
    "metrics = [tf.keras.metrics.CategoricalAccuracy(name=\"cat_accuracy\"),\n",
    "           sm.metrics.IOUScore(threshold=0.5, class_indexes=[1,2,3,4,5,6,7,8]),\n",
    "           sm.metrics.FScore(threshold=0.5, class_indexes=[1,2,3,4,5,6,7,8]),\n",
    "           tf.keras.metrics.MeanIoU(num_classes=n_classes)]\n",
    "\n",
    "\n",
    "# Load test data\n",
    "test_img = np.load(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/CV5_test_imgs_all_patch.npy\")\n",
    "test_mask = np.load(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/CV5_test_mask_all_patch.npy\")\n",
    "\n",
    "\n",
    "with tf.device('/cpu:0'):  \n",
    "    ## Load U-net backbone\n",
    "    backbone_model = keras.models.load_model(\"//resstore.unibe.ch/sowi_research_bahr/u_net_models/model_u_%s_image_net_weights.hdf5\" %m)\n",
    "    \n",
    "    \n",
    "    ## Define loss    \n",
    "    dice_loss = sm.losses.DiceLoss(class_indexes=[1,2,3,4,5,6,7,8]) \n",
    "    \n",
    "\n",
    "    ## Define optimizer\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=decay)\n",
    "    \n",
    "    \n",
    "    ## Compile model\n",
    "    backbone_model.compile(optimizer=optimizer,\n",
    "                            loss=dice_loss, \n",
    "                            metrics=metrics)\n",
    "    \n",
    "    \n",
    "    ## Load model weights\n",
    "    backbone_model.load_weights(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/u_net_models_mc/u_net_%s\" %i)\n",
    "    \n",
    "\n",
    "## Make predictions on test data\n",
    "pred_mask_CV5 = []\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    for i in range(0, len(test_img)):\n",
    "        pred_mask_CV5.append(backbone_model.predict(np.expand_dims(test_img[i], axis=0)))\n",
    "        \n",
    "        \n",
    "## get model's performance\n",
    "IoU_CV5 = get_IoU_by_class(test_mask, np.array(pred_mask_CV5)[:,0,:,:,:], [1,2,3,4,5,6,7,8])\n",
    "F1_CV5 = get_F1score_by_class(test_mask, np.array(pred_mask_CV5)[:,0,:,:,:], [1,2,3,4,5,6,7,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963a77e2-0549-43bb-8758-f62a38e8bbe4",
   "metadata": {},
   "source": [
    "#### Averaging model performance over folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7f82d6d-d6c6-4d84-af0d-d8273efbb901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bareland', 0.5369794186902821),\n",
       " ('rangeland', 0.5381745709464776),\n",
       " ('developed space', 0.5621222821505544),\n",
       " ('road', 0.6209278440719516),\n",
       " ('tree', 0.6944119354642113),\n",
       " ('water', 0.8549387903516056),\n",
       " ('agriculture land', 0.7774431699644316),\n",
       " ('buildings', 0.7984762882616199),\n",
       " ('avg', 0.6729342874876418)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IoU score\n",
    "IoU_CV_avg = list(zip(IoU_CV1,IoU_CV2,IoU_CV3,IoU_CV4,IoU_CV5))\n",
    "list(zip([\"bareland\",\"rangeland\",\"developed space\",\"road\",\"tree\",\"water\",\"agriculture land\",\"buildings\",\"avg\"], [np.mean(x) for x in IoU_CV_avg]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67677f93-17a5-4c18-90f6-4ab617e162a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bareland', 0.6982733088966855),\n",
       " ('rangeland', 0.699729834134267),\n",
       " ('developed space', 0.7196404188953878),\n",
       " ('road', 0.7661225501391034),\n",
       " ('tree', 0.8196324560082179),\n",
       " ('water', 0.9217879205235944),\n",
       " ('agriculture land', 0.8747812585017284),\n",
       " ('buildings', 0.8879468964032171),\n",
       " ('avg', 0.7984893304377751)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F1 score\n",
    "F1_CV_avg = list(zip(F1_CV1,F1_CV2,F1_CV3,F1_CV4,F1_CV5))\n",
    "list(zip([\"bareland\",\"rangeland\",\"developed space\",\"road\",\"tree\",\"water\",\"agriculture land\",\"buildings\",\"avg\"], [np.mean(x) for x in F1_CV_avg]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e779181-a0bc-430b-ac3b-7fc48efefa0b",
   "metadata": {},
   "source": [
    "### Model's performance with TTA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6513c2ed-524d-48e2-93c9-3e04c563f7b4",
   "metadata": {},
   "source": [
    "Test time augmentation is used to further increase the model's performance. The regular aspect ration of the image is used, a left-right, up-down, and a combination of left-rigth and up-down flip are used for augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bae9e12-e03b-4e85-bab8-0d2c1647f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_time_augmentation(imgs):\n",
    "    pred_mask=[]\n",
    "    \n",
    "    for i in range(0, len(imgs)):\n",
    "        p0 = backbone_model.predict(np.expand_dims(imgs[i], axis=0))[0]\n",
    "        \n",
    "        p1 = backbone_model.predict(np.expand_dims(np.fliplr(imgs[i]), axis=0))[0]\n",
    "        p1 = np.fliplr(p1)\n",
    "        \n",
    "        p2 = backbone_model.predict(np.expand_dims(np.flipud(imgs[i]), axis=0))[0]\n",
    "        p2 = np.flipud(p2)\n",
    "        \n",
    "        p3 = backbone_model.predict(np.expand_dims(np.fliplr(np.flipud(imgs[i])), axis=0))[0]\n",
    "        p3 = np.fliplr(np.flipud(p3))\n",
    "        \n",
    "        avg_pred = (p0 + p1 + p2 + p3)/4\n",
    "        pred_mask.append(avg_pred)\n",
    "    return pred_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba08391d-9d72-4f23-968e-11bb9fda43a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Model trained on CV1 (TTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e302588-a5ca-477e-a777-047dfb6e3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = \"effnetb4_focc_jacc_10\" # model name\n",
    "m = \"effnetb4\" # name backbone\n",
    "batchsize = 8\n",
    "n_classes = 9\n",
    "lr = 0.0001\n",
    "decay = 0.000001\n",
    "\n",
    "\n",
    "metrics = [tf.keras.metrics.CategoricalAccuracy(name=\"cat_accuracy\"),\n",
    "           sm.metrics.IOUScore(threshold=0.5, class_indexes=[1,2,3,4,5,6,7,8]),\n",
    "           sm.metrics.FScore(threshold=0.5, class_indexes=[1,2,3,4,5,6,7,8]),\n",
    "           tf.keras.metrics.MeanIoU(num_classes=n_classes)]\n",
    "\n",
    "\n",
    "# Load test data\n",
    "test_img = np.load(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/CV1_test_imgs_all_patch.npy\")\n",
    "test_mask = np.load(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/CV1_test_mask_all_patch.npy\")\n",
    "\n",
    "\n",
    "with tf.device('/cpu:0'):  \n",
    "    ## Load U-net backbone\n",
    "    backbone_model = keras.models.load_model(\"//resstore.unibe.ch/sowi_research_bahr/u_net_models/model_u_%s_image_net_weights.hdf5\" %m)\n",
    "    \n",
    "    \n",
    "    ## Define loss    \n",
    "    dice_loss = sm.losses.DiceLoss(class_indexes=[1,2,3,4,5,6,7,8]) \n",
    "    \n",
    "\n",
    "    ## Define optimizer\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=decay)\n",
    "    \n",
    "    \n",
    "    ## Compile model\n",
    "    backbone_model.compile(optimizer=optimizer,\n",
    "                            loss=dice_loss, \n",
    "                            metrics=metrics)\n",
    "    \n",
    "    \n",
    "    ## Load model weights\n",
    "    backbone_model.load_weights(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/u_net_models_mc/u_net_%s\" %i)\n",
    "    \n",
    "\n",
    "## Make predictions on test data\n",
    "with tf.device('/gpu:0'):\n",
    "    pred_mask_CV1 = test_time_augmentation(test_img)\n",
    "        \n",
    "        \n",
    "## get model's performance\n",
    "IoU_CV1 = get_IoU_by_class(test_mask, np.array(pred_mask_CV1)[:,:,:,:], [1,2,3,4,5,6,7,8])\n",
    "F1_CV1 = get_F1score_by_class(test_mask, np.array(pred_mask_CV1)[:,:,:,:], [1,2,3,4,5,6,7,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b617c4a2-6e86-49c8-8988-3f12be9ffff6",
   "metadata": {},
   "source": [
    "#### Model trained on CV2 (TTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc53f375-5d5d-4f64-ad15-944b29324cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.weight_decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.2.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.2.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.3.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.3.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.4.total_confusion_matrix\n"
     ]
    }
   ],
   "source": [
    "i = \"effnetb4_focc_jacc_11\" # model name\n",
    "m = \"effnetb4\" # name backbone\n",
    "batchsize = 8\n",
    "n_classes = 9\n",
    "lr = 0.0001\n",
    "decay = 0.000001\n",
    "\n",
    "\n",
    "metrics = [tf.keras.metrics.CategoricalAccuracy(name=\"cat_accuracy\"),\n",
    "           sm.metrics.IOUScore(threshold=0.5, class_indexes=[1,2,3,4,5,6,7,8]),\n",
    "           sm.metrics.FScore(threshold=0.5, class_indexes=[1,2,3,4,5,6,7,8]),\n",
    "           tf.keras.metrics.MeanIoU(num_classes=n_classes)]\n",
    "\n",
    "\n",
    "# Load test data\n",
    "test_img = np.load(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/CV2_test_imgs_all_patch.npy\")\n",
    "test_mask = np.load(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/CV2_test_mask_all_patch.npy\")\n",
    "\n",
    "\n",
    "with tf.device('/cpu:0'):  \n",
    "    ## Load U-net backbone\n",
    "    backbone_model = keras.models.load_model(\"//resstore.unibe.ch/sowi_research_bahr/u_net_models/model_u_%s_image_net_weights.hdf5\" %m)\n",
    "    \n",
    "    \n",
    "    ## Define loss    \n",
    "    dice_loss = sm.losses.DiceLoss(class_indexes=[1,2,3,4,5,6,7,8])\n",
    "    \n",
    "\n",
    "    ## Define optimizer\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=decay)\n",
    "    \n",
    "    \n",
    "    ## Compile model\n",
    "    backbone_model.compile(optimizer=optimizer,\n",
    "                            loss=dice_loss, \n",
    "                            metrics=metrics)\n",
    "    \n",
    "    \n",
    "    ## Load model weights\n",
    "    backbone_model.load_weights(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/u_net_models_mc/u_net_%s\" %i)\n",
    "    \n",
    "\n",
    "## Make predictions on test data\n",
    "pred_mask_CV2 = []\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    pred_mask_CV2 = test_time_augmentation(test_img)\n",
    "        \n",
    "        \n",
    "## get model's performance\n",
    "IoU_CV2 = get_IoU_by_class(test_mask, np.array(pred_mask_CV2)[:,:,:,:], [1,2,3,4,5,6,7,8])\n",
    "F1_CV2 = get_F1score_by_class(test_mask, np.array(pred_mask_CV2)[:,:,:,:], [1,2,3,4,5,6,7,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06200c58-48a8-4ae6-b9c8-5ceffdc19925",
   "metadata": {},
   "source": [
    "#### Model trained on CV3 (TTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9916dafb-a449-40cb-ae79-ac2c04c4ba57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.weight_decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.2.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.2.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.3.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.3.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.4.total_confusion_matrix\n"
     ]
    }
   ],
   "source": [
    "i = \"effnetb4_focc_jacc_12\" # model name\n",
    "m = \"effnetb4\" # name backbone\n",
    "batchsize = 8\n",
    "n_classes = 9\n",
    "lr = 0.0001\n",
    "decay = 0.000001\n",
    "\n",
    "\n",
    "metrics = [tf.keras.metrics.CategoricalAccuracy(name=\"cat_accuracy\"),\n",
    "           sm.metrics.IOUScore(threshold=0.5, class_indexes=[1,2,3,4,5,6,7,8]),\n",
    "           sm.metrics.FScore(threshold=0.5, class_indexes=[1,2,3,4,5,6,7,8]),\n",
    "           tf.keras.metrics.MeanIoU(num_classes=n_classes)]\n",
    "\n",
    "\n",
    "# Load test data\n",
    "test_img = np.load(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/CV3_test_imgs_all_patch.npy\")\n",
    "test_mask = np.load(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/CV3_test_mask_all_patch.npy\")\n",
    "\n",
    "\n",
    "with tf.device('/cpu:0'):  \n",
    "    ## Load U-net backbone\n",
    "    backbone_model = keras.models.load_model(\"//resstore.unibe.ch/sowi_research_bahr/u_net_models/model_u_%s_image_net_weights.hdf5\" %m)\n",
    "    \n",
    "    \n",
    "    ## Define loss    \n",
    "    dice_loss = sm.losses.DiceLoss(class_indexes=[1,2,3,4,5,6,7,8]) \n",
    "    \n",
    "\n",
    "    ## Define optimizer\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=decay)\n",
    "    \n",
    "    \n",
    "    ## Compile model\n",
    "    backbone_model.compile(optimizer=optimizer,\n",
    "                            loss=dice_loss, \n",
    "                            metrics=metrics)\n",
    "    \n",
    "    \n",
    "    ## Load model weights\n",
    "    backbone_model.load_weights(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/u_net_models_mc/u_net_%s\" %i)\n",
    "    \n",
    "\n",
    "## Make predictions on test data\n",
    "with tf.device('/gpu:0'):\n",
    "    pred_mask_CV3 = test_time_augmentation(test_img)\n",
    "        \n",
    "        \n",
    "## get model's performance\n",
    "IoU_CV3 = get_IoU_by_class(test_mask, np.array(pred_mask_CV3)[:,:,:,:], [1,2,3,4,5,6,7,8])\n",
    "F1_CV3 = get_F1score_by_class(test_mask, np.array(pred_mask_CV3)[:,:,:,:], [1,2,3,4,5,6,7,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a08258-417b-4b3f-9428-3936966e30a3",
   "metadata": {},
   "source": [
    "#### Model trained on CV4 (TTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d122b5a8-e3c4-41af-b7af-c1c8e7feae73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.weight_decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.2.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.2.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.3.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.3.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.4.total_confusion_matrix\n"
     ]
    }
   ],
   "source": [
    "i = \"effnetb4_focc_jacc_13\" # model name\n",
    "m = \"effnetb4\" # name backbone\n",
    "batchsize = 8\n",
    "n_classes = 9\n",
    "lr = 0.0001\n",
    "decay = 0.000001\n",
    "\n",
    "\n",
    "metrics = [tf.keras.metrics.CategoricalAccuracy(name=\"cat_accuracy\"),\n",
    "           sm.metrics.IOUScore(threshold=0.5, class_indexes=[1,2,3,4,5,6,7,8]),\n",
    "           sm.metrics.FScore(threshold=0.5, class_indexes=[1,2,3,4,5,6,7,8]),\n",
    "           tf.keras.metrics.MeanIoU(num_classes=n_classes)]\n",
    "\n",
    "\n",
    "# Load test data\n",
    "test_img = np.load(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/CV4_test_imgs_all_patch.npy\")\n",
    "test_mask = np.load(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/CV4_test_mask_all_patch.npy\")\n",
    "\n",
    "\n",
    "with tf.device('/cpu:0'):  \n",
    "    ## Load U-net backbone\n",
    "    backbone_model = keras.models.load_model(\"//resstore.unibe.ch/sowi_research_bahr/u_net_models/model_u_%s_image_net_weights.hdf5\" %m)\n",
    "    \n",
    "    \n",
    "    ## Define loss    \n",
    "    dice_loss = sm.losses.DiceLoss(class_indexes=[1,2,3,4,5,6,7,8]) \n",
    "    \n",
    "\n",
    "    ## Define optimizer\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=decay)\n",
    "    \n",
    "    \n",
    "    ## Compile model\n",
    "    backbone_model.compile(optimizer=optimizer,\n",
    "                            loss=dice_loss, \n",
    "                            metrics=metrics)\n",
    "    \n",
    "    \n",
    "    ## Load model weights\n",
    "    backbone_model.load_weights(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/u_net_models_mc/u_net_%s\" %i)\n",
    "    \n",
    "\n",
    "## Make predictions on test data\n",
    "with tf.device('/gpu:0'):\n",
    "    pred_mask_CV4 = test_time_augmentation(test_img)\n",
    "        \n",
    "        \n",
    "## get model's performance\n",
    "IoU_CV4 = get_IoU_by_class(test_mask, np.array(pred_mask_CV4)[:,:,:,:], [1,2,3,4,5,6,7,8])\n",
    "F1_CV4 = get_F1score_by_class(test_mask, np.array(pred_mask_CV4)[:,:,:,:], [1,2,3,4,5,6,7,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dad6699-dbab-4acc-be74-4cf1f5f9b3f6",
   "metadata": {},
   "source": [
    "#### Model trained on CV5 (TTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02565ce9-9455-45b2-a2a3-7dc417fedd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.weight_decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.2.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.2.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.3.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.3.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.4.total_confusion_matrix\n"
     ]
    }
   ],
   "source": [
    "i = \"effnetb4_focc_jacc_13\" # model name\n",
    "m = \"effnetb4\" # name backbone\n",
    "batchsize = 8\n",
    "n_classes = 9\n",
    "lr = 0.0001\n",
    "decay = 0.000001\n",
    "\n",
    "\n",
    "metrics = [tf.keras.metrics.CategoricalAccuracy(name=\"cat_accuracy\"),\n",
    "           sm.metrics.IOUScore(threshold=0.5, class_indexes=[1,2,3,4,5,6,7,8]),\n",
    "           sm.metrics.FScore(threshold=0.5, class_indexes=[1,2,3,4,5,6,7,8]),\n",
    "           tf.keras.metrics.MeanIoU(num_classes=n_classes)]\n",
    "\n",
    "\n",
    "# Load test data\n",
    "test_img = np.load(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/CV5_test_imgs_all_patch.npy\")\n",
    "test_mask = np.load(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/CV5_test_mask_all_patch.npy\")\n",
    "\n",
    "\n",
    "with tf.device('/cpu:0'):  \n",
    "    ## Load U-net backbone\n",
    "    backbone_model = keras.models.load_model(\"//resstore.unibe.ch/sowi_research_bahr/u_net_models/model_u_%s_image_net_weights.hdf5\" %m)\n",
    "    \n",
    "    \n",
    "    ## Define loss    \n",
    "    dice_loss = sm.losses.DiceLoss(class_indexes=[1,2,3,4,5,6,7,8]) \n",
    "    \n",
    "\n",
    "    ## Define optimizer\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=decay)\n",
    "    \n",
    "    \n",
    "    ## Compile model\n",
    "    backbone_model.compile(optimizer=optimizer,\n",
    "                            loss=dice_loss, \n",
    "                            metrics=metrics)\n",
    "    \n",
    "    \n",
    "    ## Load model weights\n",
    "    backbone_model.load_weights(\"//resstore.unibe.ch/sowi_research_bahr/Data_sets/image_segmentation/u_net_models_mc/u_net_%s\" %i)\n",
    "    \n",
    "\n",
    "## Make predictions on test data\n",
    "with tf.device('/gpu:0'):\n",
    "    pred_mask_CV5 = test_time_augmentation(test_img)\n",
    "        \n",
    "        \n",
    "## get model's performance\n",
    "IoU_CV5 = get_IoU_by_class(test_mask, np.array(pred_mask_CV5)[:,:,:,:], [1,2,3,4,5,6,7,8])\n",
    "F1_CV5 = get_F1score_by_class(test_mask, np.array(pred_mask_CV5)[:,:,:,:], [1,2,3,4,5,6,7,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595cb2bc-bfe6-4977-88cd-aa87d987533b",
   "metadata": {},
   "source": [
    "#### Averaging model performance over folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54d0252f-854f-4366-b25b-226903a1ae1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bareland', 0.574246089289109),\n",
       " ('rangeland', 0.5601631148322136),\n",
       " ('developed space', 0.5819971536330757),\n",
       " ('road', 0.6483327899588631),\n",
       " ('tree', 0.7063658270467628),\n",
       " ('water', 0.8584730536298402),\n",
       " ('agriculture land', 0.7991920289133622),\n",
       " ('buildings', 0.8094321748684934),\n",
       " ('avg', 0.692275279021465)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IoU score\n",
    "IoU_CV_avg = list(zip(IoU_CV1,IoU_CV2,IoU_CV3,IoU_CV4,IoU_CV5))\n",
    "list(zip([\"bareland\",\"rangeland\",\"developed space\",\"road\",\"tree\",\"water\",\"agriculture land\",\"buildings\",\"avg\"], [np.mean(x) for x in IoU_CV_avg]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a84ee01-7b43-4037-93d0-482095f1ca9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bareland', 0.7261333887151014),\n",
       " ('rangeland', 0.7176658291571935),\n",
       " ('developed space', 0.7355145850373405),\n",
       " ('road', 0.7863713814268944),\n",
       " ('tree', 0.8278125742565324),\n",
       " ('water', 0.9238040581618673),\n",
       " ('agriculture land', 0.8879654779618887),\n",
       " ('buildings', 0.89465578898178),\n",
       " ('avg', 0.8124903854623249)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F1 score\n",
    "F1_CV_avg = list(zip(F1_CV1,F1_CV2,F1_CV3,F1_CV4,F1_CV5))\n",
    "list(zip([\"bareland\",\"rangeland\",\"developed space\",\"road\",\"tree\",\"water\",\"agriculture land\",\"buildings\",\"avg\"], [np.mean(x) for x in F1_CV_avg]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
